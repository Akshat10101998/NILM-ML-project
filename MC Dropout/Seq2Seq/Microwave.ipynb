{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilmtk import DataSet\n",
    "from nilmtk.utils import print_dict\n",
    "\n",
    "redd = DataSet('converted_REDD.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nilmtk.disaggregate import Disaggregator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "class SequenceLengthError(Exception):\n",
    "    pass\n",
    "\n",
    "class ApplianceNotFoundError(Exception):\n",
    "    pass\n",
    "\n",
    "class MonteCarloDropout(Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class Seq2Seq(Disaggregator):\n",
    "\n",
    "    def __init__(self, params):\n",
    "\n",
    "        self.MODEL_NAME = \"Seq2Seq\"\n",
    "        self.file_prefix = \"{}-temp-weights\".format(self.MODEL_NAME.lower())\n",
    "        self.chunk_wise_training = params.get('chunk_wise_training',False)\n",
    "        self.sequence_length = params.get('sequence_length',99)\n",
    "        self.n_epochs = params.get('n_epochs', 10)\n",
    "        self.models = OrderedDict()\n",
    "        self.mains_mean = 1800\n",
    "        self.mains_std = 600\n",
    "        self.batch_size = params.get('batch_size',512)\n",
    "        self.appliance_params = params.get('appliance_params',{})\n",
    "        if self.sequence_length%2==0:\n",
    "            print (\"Sequence length should be odd!\")\n",
    "            raise (SequenceLengthError)\n",
    "\n",
    "    def partial_fit(self, train_main, train_appliances, do_preprocessing=True, current_epoch=0, **load_kwargs):\n",
    "        print(\"...............Seq2Seq partial_fit running...............\")\n",
    "        if len(self.appliance_params) == 0:\n",
    "            self.set_appliance_params(train_appliances)\n",
    "\n",
    "        if do_preprocessing:\n",
    "            train_main, train_appliances = self.call_preprocessing(\n",
    "                train_main, train_appliances, 'train')\n",
    "\n",
    "        train_main = pd.concat(train_main, axis=0)\n",
    "        train_main = train_main.values.reshape((-1, self.sequence_length, 1))\n",
    "        new_train_appliances = []\n",
    "        for app_name, app_dfs in train_appliances:\n",
    "            app_df = pd.concat(app_dfs, axis=0)\n",
    "            app_df_values = app_df.values.reshape((-1, self.sequence_length))\n",
    "            new_train_appliances.append((app_name, app_df_values))\n",
    "\n",
    "        train_appliances = new_train_appliances\n",
    "        for appliance_name, power in train_appliances:\n",
    "            if appliance_name not in self.models:\n",
    "                print(\"First model training for \", appliance_name)\n",
    "                self.models[appliance_name] = self.return_network()\n",
    "            else:\n",
    "                print(\"Started Retraining model for \", appliance_name)\n",
    "\n",
    "            model = self.models[appliance_name]\n",
    "            if train_main.size > 0:\n",
    "                # Sometimes chunks can be empty after dropping NANS\n",
    "                if len(train_main) > 10:\n",
    "                    # Do validation when you have sufficient samples\n",
    "                    filepath = self.file_prefix + \"-{}-epoch{}.h5\".format(\n",
    "                            \"_\".join(appliance_name.split()),\n",
    "                            current_epoch,\n",
    "                    )\n",
    "                    checkpoint = ModelCheckpoint(filepath,monitor='val_loss',verbose=1,save_best_only=True,mode='min')\n",
    "                    model.fit(\n",
    "                            train_main, power,\n",
    "                            validation_split=.15,\n",
    "                            epochs=self.n_epochs,\n",
    "                            batch_size=self.batch_size,\n",
    "                            callbacks=[ checkpoint ],\n",
    "                    )\n",
    "                    model.load_weights(filepath)\n",
    "\n",
    "                    \n",
    "    def disaggregate_chunk(self,test_main_list,model=None,do_preprocessing=True):\n",
    "        if model is not None:\n",
    "            self.models = model\n",
    "\n",
    "        if do_preprocessing:\n",
    "            test_main_list = self.call_preprocessing(\n",
    "                test_main_list, submeters_lst=None, method='test')\n",
    "\n",
    "        test_predictions = []\n",
    "        for test_mains_df in test_main_list:\n",
    "        \n",
    "            disggregation_dict = {}\n",
    "            test_main_array = test_mains_df.values.reshape((-1, self.sequence_length, 1))\n",
    "            \n",
    "            all_preds = []\n",
    "            for appliance in self.models:\n",
    "                \n",
    "                for i in range(100):\n",
    "                    \n",
    "                    if i%10==0:\n",
    "                        print(i)\n",
    "                        \n",
    "                    prediction = []\n",
    "                    model = self.models[appliance]\n",
    "                    prediction = model.predict(test_main_array ,batch_size=self.batch_size)\n",
    "\n",
    "                    #####################\n",
    "                    # This block is for creating the average of predictions over the different sequences\n",
    "                    # the counts_arr keeps the number of times a particular timestamp has occured\n",
    "                    # the sum_arr keeps the number of times a particular timestamp has occured\n",
    "                    # the predictions are summed for  agiven time, and is divided by the number of times it has occured\n",
    "\n",
    "                    l = self.sequence_length\n",
    "                    n = len(prediction) + l - 1\n",
    "                    sum_arr = np.zeros((n))\n",
    "                    counts_arr = np.zeros((n))\n",
    "                    o = len(sum_arr)\n",
    "                    for i in range(len(prediction)):\n",
    "                        sum_arr[i:i + l] += prediction[i].flatten()\n",
    "                        counts_arr[i:i + l] += 1\n",
    "                    for i in range(len(sum_arr)):\n",
    "                        sum_arr[i] = sum_arr[i] / counts_arr[i]\n",
    "\n",
    "                    #################\n",
    "                    prediction = self.appliance_params[appliance]['mean'] + (sum_arr * self.appliance_params[appliance]['std'])\n",
    "                    valid_predictions = prediction.flatten()\n",
    "                    valid_predictions = np.where(valid_predictions > 0, valid_predictions, 0)\n",
    "                    \n",
    "                    all_preds.append(valid_predictions)\n",
    "                \n",
    "                valid_predictions = np.mean(all_preds, axis=0)\n",
    "                var_predictions = np.std(all_preds, axis=0)\n",
    "                    \n",
    "                df = pd.Series(valid_predictions)\n",
    "                disggregation_dict[appliance] = df\n",
    "                \n",
    "            results = pd.DataFrame(disggregation_dict, dtype='float32')\n",
    "            test_predictions.append(results)\n",
    "            \n",
    "            df_mean = pd.DataFrame(valid_predictions)\n",
    "            df_var = pd.DataFrame(var_predictions)\n",
    "            \n",
    "            df_mean.to_csv('mean_Microwave_s2s.csv')\n",
    "            df_var.to_csv('var_Microwave_s2s.csv')\n",
    "\n",
    "        return test_predictions\n",
    "\n",
    "    def return_network(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        # 1D Conv\n",
    "        model.add(Conv1D(30,10,activation=\"relu\",input_shape=(self.sequence_length,1),strides=2))\n",
    "        model.add(Conv1D(30, 8, activation='relu', strides=2))\n",
    "        model.add(Conv1D(40, 6, activation='relu', strides=1))\n",
    "        model.add(Conv1D(50, 5, activation='relu', strides=1))\n",
    "#         model.add(Dropout(.2))\n",
    "        model.add(MonteCarloDropout(.2))\n",
    "        model.add(Conv1D(50, 5, activation='relu', strides=1))\n",
    "#         model.add(Dropout(.2))\n",
    "        model.add(MonteCarloDropout(.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "#         model.add(Dropout(.2))\n",
    "        model.add(MonteCarloDropout(.2))\n",
    "        model.add(Dense(self.sequence_length))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def call_preprocessing(self, mains_lst, submeters_lst, method):\n",
    "\n",
    "        if method == 'train':            \n",
    "            processed_mains_lst = []\n",
    "            for mains in mains_lst:\n",
    "                new_mains = mains.values.flatten()\n",
    "                n = self.sequence_length\n",
    "                units_to_pad = n // 2\n",
    "                new_mains = np.pad(new_mains, (units_to_pad,units_to_pad),'constant',constant_values = (0,0))\n",
    "                new_mains = np.array([new_mains[i:i + n] for i in range(len(new_mains) - n + 1)])\n",
    "                new_mains = (new_mains - self.mains_mean) / self.mains_std\n",
    "                processed_mains_lst.append(pd.DataFrame(new_mains))\n",
    "            #new_mains = pd.DataFrame(new_mains)\n",
    "            appliance_list = []\n",
    "            for app_index, (app_name, app_df_lst) in enumerate(submeters_lst):\n",
    "\n",
    "                if app_name in self.appliance_params:\n",
    "                    app_mean = self.appliance_params[app_name]['mean']\n",
    "                    app_std = self.appliance_params[app_name]['std']\n",
    "                else:\n",
    "                    print (\"Parameters for \", app_name ,\" were not found!\")\n",
    "                    raise ApplianceNotFoundError()\n",
    "\n",
    "\n",
    "                processed_app_dfs = []\n",
    "                for app_df in app_df_lst:                    \n",
    "                    new_app_readings = app_df.values.flatten()\n",
    "                    new_app_readings = np.pad(new_app_readings, (units_to_pad,units_to_pad),'constant',constant_values = (0,0))\n",
    "                    new_app_readings = np.array([new_app_readings[i:i + n] for i in range(len(new_app_readings) - n + 1)])                    \n",
    "                    new_app_readings = (new_app_readings - app_mean) / app_std  # /self.max_val\n",
    "                    processed_app_dfs.append(pd.DataFrame(new_app_readings))\n",
    "                    \n",
    "                    \n",
    "                appliance_list.append((app_name, processed_app_dfs))\n",
    "                #new_app_readings = np.array([ new_app_readings[i:i+n] for i in range(len(new_app_readings)-n+1) ])\n",
    "                #print (new_mains.shape, new_app_readings.shape, app_name)\n",
    "\n",
    "            return processed_mains_lst, appliance_list\n",
    "\n",
    "        else:\n",
    "            processed_mains_lst = []\n",
    "            for mains in mains_lst:\n",
    "                new_mains = mains.values.flatten()\n",
    "                n = self.sequence_length\n",
    "                units_to_pad = n // 2\n",
    "                #new_mains = np.pad(new_mains, (units_to_pad,units_to_pad),'constant',constant_values = (0,0))\n",
    "                new_mains = np.array([new_mains[i:i + n] for i in range(len(new_mains) - n + 1)])\n",
    "                new_mains = (new_mains - self.mains_mean) / self.mains_std\n",
    "                new_mains = new_mains.reshape((-1, self.sequence_length))\n",
    "                processed_mains_lst.append(pd.DataFrame(new_mains))\n",
    "            return processed_mains_lst\n",
    "\n",
    "    def set_appliance_params(self,train_appliances):\n",
    "\n",
    "        for (app_name,df_list) in train_appliances:\n",
    "            l = np.array(pd.concat(df_list,axis=0))\n",
    "            app_mean = np.mean(l)\n",
    "            app_std = np.std(l)\n",
    "            if app_std<1:\n",
    "                app_std = 100\n",
    "            self.appliance_params.update({app_name:{'mean':app_mean,'std':app_std}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilmtk.dataset import DataSet\n",
    "from nilmtk.metergroup import MeterGroup\n",
    "import pandas as pd\n",
    "from nilmtk.losses import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class API():\n",
    "\n",
    "    \"\"\"\n",
    "    The API ia designed for rapid experimentation with NILM Algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,params):\n",
    "        \"\"\"\n",
    "        Initialize the API with default parameters and then start the experiment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.appliances = []\n",
    "        self.train_submeters = []\n",
    "        self.train_mains = pd.DataFrame()\n",
    "        self.test_submeters = []\n",
    "        self.test_mains = pd.DataFrame()\n",
    "        self.gt_overall = {}\n",
    "        self.pred_overall = {}\n",
    "        self.classifiers=[]\n",
    "        self.errors = []\n",
    "        self.errors_keys = []\n",
    "        self.power = params['power']\n",
    "        for elems in params['appliances']:\n",
    "            self.appliances.append(elems)\n",
    "\n",
    "        self.train_datasets_dict = params['train']['datasets']\n",
    "        self.test_datasets_dict = params['test']['datasets']\n",
    "        self.metrics = params['test']['metrics']\n",
    "        self.methods = params['methods']\n",
    "        self.sample_period = params.get(\"sample_rate\", 1)\n",
    "        self.artificial_aggregate = params.get('artificial_aggregate', False)\n",
    "        self.chunk_size = params.get('chunk_size', None)\n",
    "        self.display_predictions = params.get('display_predictions', False)\n",
    "        self.DROP_ALL_NANS = params.get(\"DROP_ALL_NANS\", True)\n",
    "        self.site_only = params.get('site_only',False)\n",
    "        self.experiment()\n",
    "        \n",
    "\n",
    "    def experiment(self):\n",
    "        \"\"\"\n",
    "        Calls the Experiments with the specified parameters\n",
    "        \"\"\"\n",
    "\n",
    "        self.store_classifier_instances()\n",
    "        d=self.train_datasets_dict\n",
    "\n",
    "        for model_name, clf in self.classifiers:\n",
    "            # If the model is a neural net, it has an attribute n_epochs, Ex: DAE, Seq2Point\n",
    "            print (\"Started training for \",clf.MODEL_NAME)\n",
    "\n",
    "            # If the model has the filename specified for loading the pretrained model, then we don't need to load training data\n",
    "\n",
    "            if hasattr(clf,'load_model_path'):\n",
    "                if clf.load_model_path:\n",
    "                    print (clf.MODEL_NAME,\" is loading the pretrained model\")\n",
    "                    continue\n",
    "\n",
    "            # if user wants to train chunk wise\n",
    "            if self.chunk_size:\n",
    "                # If the classifier supports chunk wise training\n",
    "                if clf.chunk_wise_training:\n",
    "                    # if it has an attribute n_epochs. Ex: neural nets. Then it is trained chunk wise for every wise\n",
    "                    if hasattr(clf,'n_epochs'):\n",
    "                        n_epochs = clf.n_epochs\n",
    "                        clf.n_epochs = 1\n",
    "                    else:\n",
    "                        # If it doesn't have the attribute n_epochs, this is executed. Ex: Mean, Zero\n",
    "                        n_epochs = 1\n",
    "                    # Training on those many chunks for those many epochs\n",
    "                    print (\"Chunk wise training for \",clf.MODEL_NAME)\n",
    "                    for i in range(n_epochs):\n",
    "                        self.train_chunk_wise(clf, d, i)\n",
    "\n",
    "                else:\n",
    "                    print (\"Joint training for \",clf.MODEL_NAME)\n",
    "                    self.train_jointly(clf,d)            \n",
    "\n",
    "            # if it doesn't support chunk wise training\n",
    "            else:\n",
    "                print (\"Joint training for \",clf.MODEL_NAME)\n",
    "                self.train_jointly(clf,d)            \n",
    "\n",
    "            print (\"Finished training for \",clf.MODEL_NAME)\n",
    "            clear_output()\n",
    "\n",
    "        d=self.test_datasets_dict\n",
    "\n",
    "        if self.chunk_size:\n",
    "            print (\"Chunk Wise Testing for all algorithms\")\n",
    "            # It means that, predictions can also be done on chunks\n",
    "            self.test_chunk_wise(d)\n",
    "\n",
    "        else:\n",
    "            print (\"Joint Testing for all algorithms\")\n",
    "            self.test_jointly(d)\n",
    "\n",
    "    def train_chunk_wise(self, clf, d, current_epoch):\n",
    "        \"\"\"\n",
    "        This function loads the data from buildings and datasets with the specified chunk size and trains on each of them. \n",
    "        \"\"\"\n",
    "            \n",
    "        for dataset in d:\n",
    "            # Loading the dataset\n",
    "            print(\"Loading data for \",dataset, \" dataset\")          \n",
    "            for building in d[dataset]['buildings']:\n",
    "                # Loading the building\n",
    "                train=DataSet(d[dataset]['path'])\n",
    "                print(\"Loading building ... \",building)\n",
    "                train.set_window(start=d[dataset]['buildings'][building]['start_time'],end=d[dataset]['buildings'][building]['end_time'])\n",
    "                mains_iterator = train.buildings[building].elec.mains().load(chunksize = self.chunk_size, physical_quantity='power', ac_type = self.power['mains'], sample_period=self.sample_period)\n",
    "                appliance_iterators = [train.buildings[building].elec[app_name].load(chunksize = self.chunk_size, physical_quantity='power', ac_type=self.power['appliance'], sample_period=self.sample_period) for app_name in self.appliances]\n",
    "                print(train.buildings[building].elec.mains())\n",
    "                for chunk_num,chunk in enumerate (train.buildings[building].elec.mains().load(chunksize = self.chunk_size, physical_quantity='power', ac_type = self.power['mains'], sample_period=self.sample_period)):\n",
    "                    # Loading the chunk for the specifeid building\n",
    "                    #Dummry loop for executing on outer level. Just for looping till end of a chunk\n",
    "                    print(\"Starting enumeration..........\")\n",
    "                    train_df = next(mains_iterator)\n",
    "                    appliance_readings = []\n",
    "                    for i in appliance_iterators:\n",
    "                        try:\n",
    "                            appliance_df = next(i)\n",
    "                        except StopIteration:\n",
    "                            appliance_df = pd.DataFrame()\n",
    "                        appliance_readings.append(appliance_df)\n",
    "\n",
    "                    if self.DROP_ALL_NANS:\n",
    "                        train_df, appliance_readings = self.dropna(train_df, appliance_readings)\n",
    "                    \n",
    "                    if self.artificial_aggregate:\n",
    "                        print (\"Creating an Artificial Aggregate\")\n",
    "                        train_df = pd.DataFrame(np.zeros(appliance_readings[0].shape),index = appliance_readings[0].index,columns=appliance_readings[0].columns)\n",
    "                        for app_reading in appliance_readings:\n",
    "                            train_df+=app_reading\n",
    "                    train_appliances = []\n",
    "\n",
    "                    for cnt,i in enumerate(appliance_readings):\n",
    "                        train_appliances.append((self.appliances[cnt],[i]))\n",
    "\n",
    "                    self.train_mains = [train_df]\n",
    "                    self.train_submeters = train_appliances\n",
    "                    clf.partial_fit(self.train_mains, self.train_submeters, current_epoch)\n",
    "                \n",
    "\n",
    "        print(\"...............Finished the Training Process ...................\")\n",
    "\n",
    "    def test_chunk_wise(self,d):\n",
    "\n",
    "        print(\"...............Started  the Testing Process ...................\")\n",
    "\n",
    "        for dataset in d:\n",
    "            print(\"Loading data for \",dataset, \" dataset\")\n",
    "            for building in d[dataset]['buildings']:\n",
    "                test=DataSet(d[dataset]['path'])\n",
    "                test.set_window(start=d[dataset]['buildings'][building]['start_time'],end=d[dataset]['buildings'][building]['end_time'])\n",
    "                mains_iterator = test.buildings[building].elec.mains().load(chunksize = self.chunk_size, physical_quantity='power', ac_type = self.power['mains'], sample_period=self.sample_period)\n",
    "                appliance_iterators = [test.buildings[building].elec[app_name].load(chunksize = self.chunk_size, physical_quantity='power', ac_type=self.power['appliance'], sample_period=self.sample_period) for app_name in self.appliances]\n",
    "                for chunk_num,chunk in enumerate (test.buildings[building].elec.mains().load(chunksize = self.chunk_size, physical_quantity='power', ac_type = self.power['mains'], sample_period=self.sample_period)):\n",
    "                    test_df = next(mains_iterator)\n",
    "                    appliance_readings = []\n",
    "                    for i in appliance_iterators:\n",
    "                        try:\n",
    "                            appliance_df = next(i)\n",
    "                        except StopIteration:\n",
    "                            appliance_df = pd.DataFrame()\n",
    "\n",
    "                        appliance_readings.append(appliance_df)\n",
    "\n",
    "                    if self.DROP_ALL_NANS:\n",
    "                        test_df, appliance_readings = self.dropna(test_df, appliance_readings)\n",
    "\n",
    "                    if self.artificial_aggregate:\n",
    "                        print (\"Creating an Artificial Aggregate\")\n",
    "                        test_df = pd.DataFrame(np.zeros(appliance_readings[0].shape),index = appliance_readings[0].index,columns=appliance_readings[0].columns)\n",
    "                        for app_reading in appliance_readings:\n",
    "                            test_df+=app_reading\n",
    "\n",
    "                    test_appliances = []\n",
    "\n",
    "                    for cnt,i in enumerate(appliance_readings):\n",
    "                        test_appliances.append((self.appliances[cnt],[i]))\n",
    "\n",
    "                    self.test_mains = [test_df]\n",
    "                    self.test_submeters = test_appliances\n",
    "                    print(\"Results for Dataset {dataset} Building {building} Chunk {chunk_num}\".format(dataset=dataset,building=building,chunk_num=chunk_num))\n",
    "                    self.storing_key = str(dataset) + \"_\" + str(building) + \"_\" + str(chunk_num) \n",
    "                    self.call_predict(self.classifiers, test.metadata['timezone'])\n",
    "\n",
    "\n",
    "    def train_jointly(self,clf,d):\n",
    "\n",
    "        # This function has a few issues, which should be addressed soon\n",
    "        print(\"............... Loading Data for training ...................\")\n",
    "        # store the train_main readings for all buildings\n",
    "        self.train_mains = []\n",
    "        self.train_submeters = [[] for i in range(len(self.appliances))]\n",
    "        for dataset in d:\n",
    "            print(\"Loading data for \",dataset, \" dataset\")\n",
    "            train=DataSet(d[dataset]['path'])\n",
    "            for building in d[dataset]['buildings']:\n",
    "                print(\"Loading building ... \",building)\n",
    "                train.set_window(start=d[dataset]['buildings'][building]['start_time'],end=d[dataset]['buildings'][building]['end_time'])\n",
    "                train_df = next(train.buildings[building].elec.mains().load(physical_quantity='power', ac_type=self.power['mains'], sample_period=self.sample_period))\n",
    "                train_df = train_df[[list(train_df.columns)[0]]]\n",
    "                appliance_readings = []\n",
    "                \n",
    "                for appliance_name in self.appliances:\n",
    "                    appliance_df = next(train.buildings[building].elec[appliance_name].load(physical_quantity='power', ac_type=self.power['appliance'], sample_period=self.sample_period))\n",
    "                    appliance_df = appliance_df[[list(appliance_df.columns)[0]]]\n",
    "                    appliance_readings.append(appliance_df)\n",
    "\n",
    "                if self.DROP_ALL_NANS:\n",
    "                    train_df, appliance_readings = self.dropna(train_df, appliance_readings)\n",
    "\n",
    "                if self.artificial_aggregate:\n",
    "                    print (\"Creating an Artificial Aggregate\")\n",
    "                    train_df = pd.DataFrame(np.zeros(appliance_readings[0].shape),index = appliance_readings[0].index,columns=appliance_readings[0].columns)\n",
    "                    for app_reading in appliance_readings:\n",
    "                        train_df+=app_reading\n",
    "\n",
    "                self.train_mains.append(train_df)\n",
    "                for i,appliance_name in enumerate(self.appliances):\n",
    "                    self.train_submeters[i].append(appliance_readings[i])\n",
    "\n",
    "        appliance_readings = []\n",
    "        for i,appliance_name in enumerate(self.appliances):\n",
    "            appliance_readings.append((appliance_name, self.train_submeters[i]))\n",
    "\n",
    "        self.train_submeters = appliance_readings   \n",
    "\n",
    "        clf.partial_fit(self.train_mains,self.train_submeters)\n",
    "\n",
    "    \n",
    "    def test_jointly(self,d):\n",
    "        # store the test_main readings for all buildings\n",
    "        for dataset in d:\n",
    "            print(\"Loading data for \",dataset, \" dataset\")\n",
    "            test=DataSet(d[dataset]['path'])\n",
    "            for building in d[dataset]['buildings']:\n",
    "                test.set_window(start=d[dataset]['buildings'][building]['start_time'],end=d[dataset]['buildings'][building]['end_time'])\n",
    "                test_mains=next(test.buildings[building].elec.mains().load(physical_quantity='power', ac_type=self.power['mains'], sample_period=self.sample_period))\n",
    "                if self.DROP_ALL_NANS and self.site_only:\n",
    "                    test_mains, _= self.dropna(test_mains,[])\n",
    "\n",
    "                if self.site_only != True:\n",
    "                    appliance_readings=[]\n",
    "\n",
    "                    for appliance in self.appliances:\n",
    "                        test_df=next((test.buildings[building].elec[appliance].load(physical_quantity='power', ac_type=self.power['appliance'], sample_period=self.sample_period)))\n",
    "                        appliance_readings.append(test_df)\n",
    "                    \n",
    "                    if self.DROP_ALL_NANS:\n",
    "                        test_mains , appliance_readings = self.dropna(test_mains,appliance_readings)\n",
    "                \n",
    "                    if self.artificial_aggregate:\n",
    "                        print (\"Creating an Artificial Aggregate\")\n",
    "                        test_mains = pd.DataFrame(np.zeros(appliance_readings[0].shape),index = appliance_readings[0].index,columns=appliance_readings[0].columns)\n",
    "                        for app_reading in appliance_readings:\n",
    "                            test_mains+=app_reading\n",
    "                    for i, appliance_name in enumerate(self.appliances):\n",
    "                        self.test_submeters.append((appliance_name,[appliance_readings[i]]))\n",
    "\n",
    "                self.test_mains = [test_mains]\n",
    "                self.storing_key = str(dataset) + \"_\" + str(building) \n",
    "                self.call_predict(self.classifiers, test.metadata[\"timezone\"])\n",
    "\n",
    "\n",
    "    def dropna(self,mains_df, appliance_dfs=[]):\n",
    "        \"\"\"\n",
    "        Drops the missing values in the Mains reading and appliance readings and returns consistent data by copmuting the intersection\n",
    "        \"\"\"\n",
    "        print (\"Dropping missing values\")\n",
    "\n",
    "        # The below steps are for making sure that data is consistent by doing intersection across appliances\n",
    "        mains_df = mains_df.dropna()\n",
    "        ix = mains_df.index\n",
    "        mains_df = mains_df.loc[ix]\n",
    "        for i in range(len(appliance_dfs)):\n",
    "            appliance_dfs[i] = appliance_dfs[i].dropna()\n",
    "    \n",
    "        for  app_df in appliance_dfs:\n",
    "            ix = ix.intersection(app_df.index)\n",
    "        mains_df = mains_df.loc[ix]\n",
    "        new_appliances_list = []\n",
    "        for app_df in appliance_dfs:\n",
    "            new_appliances_list.append(app_df.loc[ix])\n",
    "        return mains_df,new_appliances_list\n",
    "    \n",
    "    \n",
    "    def store_classifier_instances(self):\n",
    "\n",
    "        \"\"\"\n",
    "        This function is reponsible for initializing the models with the specified model parameters\n",
    "        \"\"\"\n",
    "        for name in self.methods:\n",
    "            try:\n",
    "                                \n",
    "                clf=self.methods[name]\n",
    "                self.classifiers.append((name,clf))\n",
    "\n",
    "            except Exception as e:\n",
    "                print (\"\\n\\nThe method {model_name} specied does not exist. \\n\\n\".format(model_name=name))\n",
    "                print (e)\n",
    "    \n",
    "    def call_predict(self, classifiers, timezone):\n",
    "\n",
    "        \"\"\"\n",
    "        This functions computers the predictions on the self.test_mains using all the trained models and then compares different learn't models using the metrics specified\n",
    "        \"\"\"\n",
    "        \n",
    "        pred_overall={}\n",
    "        gt_overall={}           \n",
    "        for name,clf in classifiers:\n",
    "            gt_overall,pred_overall[name]=self.predict(clf,self.test_mains,self.test_submeters, self.sample_period, timezone)\n",
    "\n",
    "        self.gt_overall=gt_overall\n",
    "        self.pred_overall=pred_overall\n",
    "        if self.site_only != True:\n",
    "            if gt_overall.size==0:\n",
    "                print (\"No samples found in ground truth\")\n",
    "                return None\n",
    "            for metric in self.metrics:\n",
    "                try:\n",
    "                    loss_function = globals()[metric]                \n",
    "                except:\n",
    "                    print (\"Loss function \",metric, \" is not supported currently!\")\n",
    "                    continue\n",
    "\n",
    "                computed_metric={}\n",
    "                for clf_name,clf in classifiers:\n",
    "                    computed_metric[clf_name] = self.compute_loss(gt_overall, pred_overall[clf_name], loss_function)\n",
    "                computed_metric = pd.DataFrame(computed_metric)\n",
    "                print(\"............ \" ,metric,\" ..............\")\n",
    "                print(computed_metric) \n",
    "                self.errors.append(computed_metric)\n",
    "                self.errors_keys.append(self.storing_key + \"_\" + metric)\n",
    "\n",
    "\n",
    "        if self.site_only != True:\n",
    "            for i in gt_overall.columns:\n",
    "                plt.figure()\n",
    "                #plt.plot(self.test_mains[0],label='Mains reading')\n",
    "                plt.plot(gt_overall[i],label='Truth')\n",
    "                for clf in pred_overall:                \n",
    "                    plt.plot(pred_overall[clf][i],label='Predicted')\n",
    "                    plt.xticks(rotation=90)\n",
    "                plt.title(i)\n",
    "                plt.legend(loc = 'upper right')\n",
    "                plt.xlabel('Time')\n",
    "                plt.ylabel('Power (W)')\n",
    "            plt.ylim(0, 1600)\n",
    "            plt.savefig('plt_Microwave_s2s.pdf', bbox_inches = 'tight')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "    def predict(self, clf, test_elec, test_submeters, sample_period, timezone ):\n",
    "        print (\"Generating predictions for :\",clf.MODEL_NAME)        \n",
    "        \"\"\"\n",
    "        Generates predictions on the test dataset using the specified classifier.\n",
    "        \"\"\"\n",
    "        \n",
    "        # \"ac_type\" varies according to the dataset used. \n",
    "        # Make sure to use the correct ac_type before using the default parameters in this code.   \n",
    "        \n",
    "           \n",
    "        pred_list = clf.disaggregate_chunk(test_elec)\n",
    "\n",
    "        # It might not have time stamps sometimes due to neural nets\n",
    "        # It has the readings for all the appliances\n",
    "\n",
    "        concat_pred_df = pd.concat(pred_list,axis=0)\n",
    "\n",
    "        gt = {}\n",
    "        for meter,data in test_submeters:\n",
    "                concatenated_df_app = pd.concat(data,axis=1)\n",
    "                index = concatenated_df_app.index\n",
    "                gt[meter] = pd.Series(concatenated_df_app.values.flatten(),index=index)\n",
    "\n",
    "        gt_overall = pd.DataFrame(gt, dtype='float32')\n",
    "        pred = {}\n",
    "\n",
    "        if self.site_only ==True:\n",
    "            for app_name in concat_pred_df.columns:\n",
    "                app_series_values = concat_pred_df[app_name].values.flatten()\n",
    "                pred[app_name] = pd.Series(app_series_values)\n",
    "            pred_overall = pd.DataFrame(pred,dtype='float32')\n",
    "            pred_overall.plot(label=\"Pred\")\n",
    "            plt.title('Disaggregated Data')\n",
    "            plt.legend()\n",
    "\n",
    "        else:\n",
    "            for app_name in concat_pred_df.columns:\n",
    "                app_series_values = concat_pred_df[app_name].values.flatten()\n",
    "                # Neural nets do extra padding sometimes, to fit, so get rid of extra predictions\n",
    "                app_series_values = app_series_values[:len(gt_overall[app_name])]\n",
    "                pred[app_name] = pd.Series(app_series_values, index = gt_overall.index)\n",
    "            pred_overall = pd.DataFrame(pred,dtype='float32')\n",
    "        gt_df = pd.DataFrame(gt_overall)\n",
    "        pred_df = pd.DataFrame(pred_overall)\n",
    "#         print(gt_df)\n",
    "#         print(pred_df)\n",
    "        gt_df.to_csv('gt_Microwave_s2s.csv')\n",
    "        pred_df.to_csv('pred_Microwave_s2s.csv')\n",
    "        return gt_overall, pred_overall\n",
    "\n",
    "\n",
    "    # metrics\n",
    "    def compute_loss(self,gt,clf_pred, loss_function):\n",
    "        error = {}\n",
    "        for app_name in gt.columns:\n",
    "            error[app_name] = loss_function(gt[app_name],clf_pred[app_name])\n",
    "        return pd.Series(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint Testing for all algorithms\n",
      "Loading data for  REDD  dataset\n",
      "Loading data for meter ElecMeterID(instance=2, building=2, dataset='REDD')     \n",
      "Done loading data all meters for this chunk.\n",
      "Dropping missing values\n",
      "Generating predictions for : Seq2Seq\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "............  rmse  ..............\n",
      "               Seq2Seq\n",
      "dish washer  52.667613\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nilmtk.disaggregate import Mean\n",
    "from nilmtk_contrib.disaggregate import DAE, RNN, WindowGRU\n",
    "\n",
    "\n",
    "experiment = {\n",
    "  'power': {'mains': ['apparent','active'],'appliance': ['apparent','active']},\n",
    "  'sample_rate': 60,\n",
    "  'appliances': ['microwave'],\n",
    "  'methods': {'Seq2Seq':Seq2Seq({'n_epochs':50,'batch_size':32})},\n",
    "  'train': {    \n",
    "    'datasets': {\n",
    "        'REDD': {\n",
    "            'path': 'converted_REDD.h5',\n",
    "            'buildings': {\n",
    "                1: {\n",
    "                    'start_time': '2011-04-21',\n",
    "                    'end_time': '2011-05-21'\n",
    "                    },\n",
    "                2: {\n",
    "                    'start_time': '2011-04-21',\n",
    "                    'end_time': '2011-05-21'\n",
    "                    }\n",
    "                }                \n",
    "            }\n",
    "        }\n",
    "    },\n",
    "  'test': {\n",
    "    'datasets': {\n",
    "        'REDD': {\n",
    "            'path': 'converted_REDD.h5',\n",
    "            'buildings': {\n",
    "                3: {\n",
    "                    'start_time': '2011-04-21',\n",
    "                    'end_time': '2011-05-21'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'metrics':['rmse']\n",
    "    }\n",
    "}\n",
    "\n",
    "api_res = API(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_gt = pd.read_csv('gt_Microwave_s2s.csv')\n",
    "df_pred = pd.read_csv('pred_Microwave_s2s.csv')\n",
    "df_var = pd.read_csv('var_Microwave_s2s.csv')\n",
    "\n",
    "date = []\n",
    "for i in range(len(df_gt)):\n",
    "    date.append(df_gt['Unnamed: 0'][i][:10])\n",
    "plt.figure(figsize=(13,9))\n",
    "plt.plot(date, df_gt['microwave'], label = 'Truth')\n",
    "plt.plot(date, df_pred['microwave'], label = 'Predicted')\n",
    "plt.fill_between( date,\n",
    "        df_pred['microwave'] - df_var['0'] * ((3+1.)),\n",
    "        df_pred['microwave'] + df_var['0'] * ((3+1.)),\n",
    "        color=\"g\",\n",
    "        alpha=0.4)\n",
    "plt.title(\"Microwave\")\n",
    "plt.ylabel(\"Power (W)\")\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim(0, 1600)\n",
    "plt.legend(loc = 'best')\n",
    "plt.savefig('plt_Microwave_s2s.pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5365e38e87508a924dd806c38ed66b8ea6d39e6367a7129b179e0b2ec997f787"
  },
  "kernelspec": {
   "display_name": "nilm",
   "language": "python",
   "name": "nilm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
