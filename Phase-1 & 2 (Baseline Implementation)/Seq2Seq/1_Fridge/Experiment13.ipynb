{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilmtk import DataSet\n",
    "from nilmtk.utils import print_dict\n",
    "\n",
    "redd = DataSet('converted_REDD.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilmtk.dataset import DataSet\n",
    "from nilmtk.metergroup import MeterGroup\n",
    "import pandas as pd\n",
    "from nilmtk.losses import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class API():\n",
    "\n",
    "    \"\"\"\n",
    "    The API ia designed for rapid experimentation with NILM Algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,params):\n",
    "        \"\"\"\n",
    "        Initialize the API with default parameters and then start the experiment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.appliances = []\n",
    "        self.train_submeters = []\n",
    "        self.train_mains = pd.DataFrame()\n",
    "        self.test_submeters = []\n",
    "        self.test_mains = pd.DataFrame()\n",
    "        self.gt_overall = {}\n",
    "        self.pred_overall = {}\n",
    "        self.classifiers=[]\n",
    "        self.errors = []\n",
    "        self.errors_keys = []\n",
    "        self.power = params['power']\n",
    "        for elems in params['appliances']:\n",
    "            self.appliances.append(elems)\n",
    "\n",
    "        self.train_datasets_dict = params['train']['datasets']\n",
    "        self.test_datasets_dict = params['test']['datasets']\n",
    "        self.metrics = params['test']['metrics']\n",
    "        self.methods = params['methods']\n",
    "        self.sample_period = params.get(\"sample_rate\", 1)\n",
    "        self.artificial_aggregate = params.get('artificial_aggregate', False)\n",
    "        self.chunk_size = params.get('chunk_size', None)\n",
    "        self.display_predictions = params.get('display_predictions', False)\n",
    "        self.DROP_ALL_NANS = params.get(\"DROP_ALL_NANS\", True)\n",
    "        self.site_only = params.get('site_only',False)\n",
    "        self.experiment()\n",
    "        \n",
    "\n",
    "    def experiment(self):\n",
    "        \"\"\"\n",
    "        Calls the Experiments with the specified parameters\n",
    "        \"\"\"\n",
    "\n",
    "        self.store_classifier_instances()\n",
    "        d=self.train_datasets_dict\n",
    "\n",
    "        for model_name, clf in self.classifiers:\n",
    "            # If the model is a neural net, it has an attribute n_epochs, Ex: DAE, Seq2Point\n",
    "            print (\"Started training for \",clf.MODEL_NAME)\n",
    "\n",
    "            # If the model has the filename specified for loading the pretrained model, then we don't need to load training data\n",
    "\n",
    "            if hasattr(clf,'load_model_path'):\n",
    "                if clf.load_model_path:\n",
    "                    print (clf.MODEL_NAME,\" is loading the pretrained model\")\n",
    "                    continue\n",
    "\n",
    "            # if user wants to train chunk wise\n",
    "            if self.chunk_size:\n",
    "                # If the classifier supports chunk wise training\n",
    "                if clf.chunk_wise_training:\n",
    "                    # if it has an attribute n_epochs. Ex: neural nets. Then it is trained chunk wise for every wise\n",
    "                    if hasattr(clf,'n_epochs'):\n",
    "                        n_epochs = clf.n_epochs\n",
    "                        clf.n_epochs = 1\n",
    "                    else:\n",
    "                        # If it doesn't have the attribute n_epochs, this is executed. Ex: Mean, Zero\n",
    "                        n_epochs = 1\n",
    "                    # Training on those many chunks for those many epochs\n",
    "                    print (\"Chunk wise training for \",clf.MODEL_NAME)\n",
    "                    for i in range(n_epochs):\n",
    "                        self.train_chunk_wise(clf, d, i)\n",
    "\n",
    "                else:\n",
    "                    print (\"Joint training for \",clf.MODEL_NAME)\n",
    "                    self.train_jointly(clf,d)            \n",
    "\n",
    "            # if it doesn't support chunk wise training\n",
    "            else:\n",
    "                print (\"Joint training for \",clf.MODEL_NAME)\n",
    "                self.train_jointly(clf,d)            \n",
    "\n",
    "            print (\"Finished training for \",clf.MODEL_NAME)\n",
    "            clear_output()\n",
    "\n",
    "        d=self.test_datasets_dict\n",
    "\n",
    "        if self.chunk_size:\n",
    "            print (\"Chunk Wise Testing for all algorithms\")\n",
    "            # It means that, predictions can also be done on chunks\n",
    "            self.test_chunk_wise(d)\n",
    "\n",
    "        else:\n",
    "            print (\"Joint Testing for all algorithms\")\n",
    "            self.test_jointly(d)\n",
    "\n",
    "    def train_chunk_wise(self, clf, d, current_epoch):\n",
    "        \"\"\"\n",
    "        This function loads the data from buildings and datasets with the specified chunk size and trains on each of them. \n",
    "        \"\"\"\n",
    "            \n",
    "        for dataset in d:\n",
    "            # Loading the dataset\n",
    "            print(\"Loading data for \",dataset, \" dataset\")          \n",
    "            for building in d[dataset]['buildings']:\n",
    "                # Loading the building\n",
    "                train=DataSet(d[dataset]['path'])\n",
    "                print(\"Loading building ... \",building)\n",
    "                train.set_window(start=d[dataset]['buildings'][building]['start_time'],end=d[dataset]['buildings'][building]['end_time'])\n",
    "                mains_iterator = train.buildings[building].elec.mains().load(chunksize = self.chunk_size, physical_quantity='power', ac_type = self.power['mains'], sample_period=self.sample_period)\n",
    "                appliance_iterators = [train.buildings[building].elec[app_name].load(chunksize = self.chunk_size, physical_quantity='power', ac_type=self.power['appliance'], sample_period=self.sample_period) for app_name in self.appliances]\n",
    "                print(train.buildings[building].elec.mains())\n",
    "                for chunk_num,chunk in enumerate (train.buildings[building].elec.mains().load(chunksize = self.chunk_size, physical_quantity='power', ac_type = self.power['mains'], sample_period=self.sample_period)):\n",
    "                    # Loading the chunk for the specifeid building\n",
    "                    #Dummry loop for executing on outer level. Just for looping till end of a chunk\n",
    "                    print(\"Starting enumeration..........\")\n",
    "                    train_df = next(mains_iterator)\n",
    "                    appliance_readings = []\n",
    "                    for i in appliance_iterators:\n",
    "                        try:\n",
    "                            appliance_df = next(i)\n",
    "                        except StopIteration:\n",
    "                            appliance_df = pd.DataFrame()\n",
    "                        appliance_readings.append(appliance_df)\n",
    "\n",
    "                    if self.DROP_ALL_NANS:\n",
    "                        train_df, appliance_readings = self.dropna(train_df, appliance_readings)\n",
    "                    \n",
    "                    if self.artificial_aggregate:\n",
    "                        print (\"Creating an Artificial Aggregate\")\n",
    "                        train_df = pd.DataFrame(np.zeros(appliance_readings[0].shape),index = appliance_readings[0].index,columns=appliance_readings[0].columns)\n",
    "                        for app_reading in appliance_readings:\n",
    "                            train_df+=app_reading\n",
    "                    train_appliances = []\n",
    "\n",
    "                    for cnt,i in enumerate(appliance_readings):\n",
    "                        train_appliances.append((self.appliances[cnt],[i]))\n",
    "\n",
    "                    self.train_mains = [train_df]\n",
    "                    self.train_submeters = train_appliances\n",
    "                    clf.partial_fit(self.train_mains, self.train_submeters, current_epoch)\n",
    "                \n",
    "\n",
    "        print(\"...............Finished the Training Process ...................\")\n",
    "\n",
    "    def test_chunk_wise(self,d):\n",
    "\n",
    "        print(\"...............Started  the Testing Process ...................\")\n",
    "\n",
    "        for dataset in d:\n",
    "            print(\"Loading data for \",dataset, \" dataset\")\n",
    "            for building in d[dataset]['buildings']:\n",
    "                test=DataSet(d[dataset]['path'])\n",
    "                test.set_window(start=d[dataset]['buildings'][building]['start_time'],end=d[dataset]['buildings'][building]['end_time'])\n",
    "                mains_iterator = test.buildings[building].elec.mains().load(chunksize = self.chunk_size, physical_quantity='power', ac_type = self.power['mains'], sample_period=self.sample_period)\n",
    "                appliance_iterators = [test.buildings[building].elec[app_name].load(chunksize = self.chunk_size, physical_quantity='power', ac_type=self.power['appliance'], sample_period=self.sample_period) for app_name in self.appliances]\n",
    "                for chunk_num,chunk in enumerate (test.buildings[building].elec.mains().load(chunksize = self.chunk_size, physical_quantity='power', ac_type = self.power['mains'], sample_period=self.sample_period)):\n",
    "                    test_df = next(mains_iterator)\n",
    "                    appliance_readings = []\n",
    "                    for i in appliance_iterators:\n",
    "                        try:\n",
    "                            appliance_df = next(i)\n",
    "                        except StopIteration:\n",
    "                            appliance_df = pd.DataFrame()\n",
    "\n",
    "                        appliance_readings.append(appliance_df)\n",
    "\n",
    "                    if self.DROP_ALL_NANS:\n",
    "                        test_df, appliance_readings = self.dropna(test_df, appliance_readings)\n",
    "\n",
    "                    if self.artificial_aggregate:\n",
    "                        print (\"Creating an Artificial Aggregate\")\n",
    "                        test_df = pd.DataFrame(np.zeros(appliance_readings[0].shape),index = appliance_readings[0].index,columns=appliance_readings[0].columns)\n",
    "                        for app_reading in appliance_readings:\n",
    "                            test_df+=app_reading\n",
    "\n",
    "                    test_appliances = []\n",
    "\n",
    "                    for cnt,i in enumerate(appliance_readings):\n",
    "                        test_appliances.append((self.appliances[cnt],[i]))\n",
    "\n",
    "                    self.test_mains = [test_df]\n",
    "                    self.test_submeters = test_appliances\n",
    "                    print(\"Results for Dataset {dataset} Building {building} Chunk {chunk_num}\".format(dataset=dataset,building=building,chunk_num=chunk_num))\n",
    "                    self.storing_key = str(dataset) + \"_\" + str(building) + \"_\" + str(chunk_num) \n",
    "                    self.call_predict(self.classifiers, test.metadata['timezone'])\n",
    "\n",
    "\n",
    "    def train_jointly(self,clf,d):\n",
    "\n",
    "        # This function has a few issues, which should be addressed soon\n",
    "        print(\"............... Loading Data for training ...................\")\n",
    "        # store the train_main readings for all buildings\n",
    "        self.train_mains = []\n",
    "        self.train_submeters = [[] for i in range(len(self.appliances))]\n",
    "        for dataset in d:\n",
    "            print(\"Loading data for \",dataset, \" dataset\")\n",
    "            train=DataSet(d[dataset]['path'])\n",
    "            for building in d[dataset]['buildings']:\n",
    "                print(\"Loading building ... \",building)\n",
    "                train.set_window(start=d[dataset]['buildings'][building]['start_time'],end=d[dataset]['buildings'][building]['end_time'])\n",
    "                train_df = next(train.buildings[building].elec.mains().load(physical_quantity='power', ac_type=self.power['mains'], sample_period=self.sample_period))\n",
    "                train_df = train_df[[list(train_df.columns)[0]]]\n",
    "                appliance_readings = []\n",
    "                \n",
    "                for appliance_name in self.appliances:\n",
    "                    appliance_df = next(train.buildings[building].elec[appliance_name].load(physical_quantity='power', ac_type=self.power['appliance'], sample_period=self.sample_period))\n",
    "                    appliance_df = appliance_df[[list(appliance_df.columns)[0]]]\n",
    "                    appliance_readings.append(appliance_df)\n",
    "\n",
    "                if self.DROP_ALL_NANS:\n",
    "                    train_df, appliance_readings = self.dropna(train_df, appliance_readings)\n",
    "\n",
    "                if self.artificial_aggregate:\n",
    "                    print (\"Creating an Artificial Aggregate\")\n",
    "                    train_df = pd.DataFrame(np.zeros(appliance_readings[0].shape),index = appliance_readings[0].index,columns=appliance_readings[0].columns)\n",
    "                    for app_reading in appliance_readings:\n",
    "                        train_df+=app_reading\n",
    "\n",
    "                self.train_mains.append(train_df)\n",
    "                for i,appliance_name in enumerate(self.appliances):\n",
    "                    self.train_submeters[i].append(appliance_readings[i])\n",
    "\n",
    "        appliance_readings = []\n",
    "        for i,appliance_name in enumerate(self.appliances):\n",
    "            appliance_readings.append((appliance_name, self.train_submeters[i]))\n",
    "\n",
    "        self.train_submeters = appliance_readings   \n",
    "\n",
    "        clf.partial_fit(self.train_mains,self.train_submeters)\n",
    "\n",
    "    \n",
    "    def test_jointly(self,d):\n",
    "        # store the test_main readings for all buildings\n",
    "        for dataset in d:\n",
    "            print(\"Loading data for \",dataset, \" dataset\")\n",
    "            test=DataSet(d[dataset]['path'])\n",
    "            for building in d[dataset]['buildings']:\n",
    "                test.set_window(start=d[dataset]['buildings'][building]['start_time'],end=d[dataset]['buildings'][building]['end_time'])\n",
    "                test_mains=next(test.buildings[building].elec.mains().load(physical_quantity='power', ac_type=self.power['mains'], sample_period=self.sample_period))\n",
    "                if self.DROP_ALL_NANS and self.site_only:\n",
    "                    test_mains, _= self.dropna(test_mains,[])\n",
    "\n",
    "                if self.site_only != True:\n",
    "                    appliance_readings=[]\n",
    "\n",
    "                    for appliance in self.appliances:\n",
    "                        test_df=next((test.buildings[building].elec[appliance].load(physical_quantity='power', ac_type=self.power['appliance'], sample_period=self.sample_period)))\n",
    "                        appliance_readings.append(test_df)\n",
    "                    \n",
    "                    if self.DROP_ALL_NANS:\n",
    "                        test_mains , appliance_readings = self.dropna(test_mains,appliance_readings)\n",
    "                \n",
    "                    if self.artificial_aggregate:\n",
    "                        print (\"Creating an Artificial Aggregate\")\n",
    "                        test_mains = pd.DataFrame(np.zeros(appliance_readings[0].shape),index = appliance_readings[0].index,columns=appliance_readings[0].columns)\n",
    "                        for app_reading in appliance_readings:\n",
    "                            test_mains+=app_reading\n",
    "                    for i, appliance_name in enumerate(self.appliances):\n",
    "                        self.test_submeters.append((appliance_name,[appliance_readings[i]]))\n",
    "\n",
    "                self.test_mains = [test_mains]\n",
    "                self.storing_key = str(dataset) + \"_\" + str(building) \n",
    "                self.call_predict(self.classifiers, test.metadata[\"timezone\"])\n",
    "\n",
    "\n",
    "    def dropna(self,mains_df, appliance_dfs=[]):\n",
    "        \"\"\"\n",
    "        Drops the missing values in the Mains reading and appliance readings and returns consistent data by copmuting the intersection\n",
    "        \"\"\"\n",
    "        print (\"Dropping missing values\")\n",
    "\n",
    "        # The below steps are for making sure that data is consistent by doing intersection across appliances\n",
    "        mains_df = mains_df.dropna()\n",
    "        ix = mains_df.index\n",
    "        mains_df = mains_df.loc[ix]\n",
    "        for i in range(len(appliance_dfs)):\n",
    "            appliance_dfs[i] = appliance_dfs[i].dropna()\n",
    "    \n",
    "        for  app_df in appliance_dfs:\n",
    "            ix = ix.intersection(app_df.index)\n",
    "        mains_df = mains_df.loc[ix]\n",
    "        new_appliances_list = []\n",
    "        for app_df in appliance_dfs:\n",
    "            new_appliances_list.append(app_df.loc[ix])\n",
    "        return mains_df,new_appliances_list\n",
    "    \n",
    "    \n",
    "    def store_classifier_instances(self):\n",
    "\n",
    "        \"\"\"\n",
    "        This function is reponsible for initializing the models with the specified model parameters\n",
    "        \"\"\"\n",
    "        for name in self.methods:\n",
    "            try:\n",
    "                                \n",
    "                clf=self.methods[name]\n",
    "                self.classifiers.append((name,clf))\n",
    "\n",
    "            except Exception as e:\n",
    "                print (\"\\n\\nThe method {model_name} specied does not exist. \\n\\n\".format(model_name=name))\n",
    "                print (e)\n",
    "    \n",
    "    def call_predict(self, classifiers, timezone):\n",
    "\n",
    "        \"\"\"\n",
    "        This functions computers the predictions on the self.test_mains using all the trained models and then compares different learn't models using the metrics specified\n",
    "        \"\"\"\n",
    "        \n",
    "        pred_overall={}\n",
    "        gt_overall={}           \n",
    "        for name,clf in classifiers:\n",
    "            gt_overall,pred_overall[name]=self.predict(clf,self.test_mains,self.test_submeters, self.sample_period, timezone)\n",
    "\n",
    "        self.gt_overall=gt_overall\n",
    "        self.pred_overall=pred_overall\n",
    "        if self.site_only != True:\n",
    "            if gt_overall.size==0:\n",
    "                print (\"No samples found in ground truth\")\n",
    "                return None\n",
    "            for metric in self.metrics:\n",
    "                try:\n",
    "                    loss_function = globals()[metric]                \n",
    "                except:\n",
    "                    print (\"Loss function \",metric, \" is not supported currently!\")\n",
    "                    continue\n",
    "\n",
    "                computed_metric={}\n",
    "                for clf_name,clf in classifiers:\n",
    "                    computed_metric[clf_name] = self.compute_loss(gt_overall, pred_overall[clf_name], loss_function)\n",
    "                computed_metric = pd.DataFrame(computed_metric)\n",
    "                print(\"............ \" ,metric,\" ..............\")\n",
    "                print(computed_metric) \n",
    "                self.errors.append(computed_metric)\n",
    "                self.errors_keys.append(self.storing_key + \"_\" + metric)\n",
    "\n",
    "\n",
    "        if self.display_predictions:\n",
    "            if self.site_only != True:\n",
    "                for i in gt_overall.columns:\n",
    "                    plt.figure()\n",
    "                    #plt.plot(self.test_mains[0],label='Mains reading')\n",
    "                    plt.plot(gt_overall[i],label='Truth')\n",
    "                    for clf in pred_overall:                \n",
    "                        plt.plot(pred_overall[clf][i],label=clf)\n",
    "                        plt.xticks(rotation=90)\n",
    "                    plt.title(i)\n",
    "                    plt.legend()\n",
    "                    plt.xlabel('Time')\n",
    "                    plt.ylabel('Power (W)')\n",
    "                plt.show()\n",
    "        \n",
    "    def predict(self, clf, test_elec, test_submeters, sample_period, timezone ):\n",
    "        print (\"Generating predictions for :\",clf.MODEL_NAME)        \n",
    "        \"\"\"\n",
    "        Generates predictions on the test dataset using the specified classifier.\n",
    "        \"\"\"\n",
    "        \n",
    "        # \"ac_type\" varies according to the dataset used. \n",
    "        # Make sure to use the correct ac_type before using the default parameters in this code.   \n",
    "        \n",
    "           \n",
    "        pred_list = clf.disaggregate_chunk(test_elec)\n",
    "\n",
    "        # It might not have time stamps sometimes due to neural nets\n",
    "        # It has the readings for all the appliances\n",
    "\n",
    "        concat_pred_df = pd.concat(pred_list,axis=0)\n",
    "\n",
    "        gt = {}\n",
    "        for meter,data in test_submeters:\n",
    "                concatenated_df_app = pd.concat(data,axis=1)\n",
    "                index = concatenated_df_app.index\n",
    "                gt[meter] = pd.Series(concatenated_df_app.values.flatten(),index=index)\n",
    "\n",
    "        gt_overall = pd.DataFrame(gt, dtype='float32')\n",
    "        pred = {}\n",
    "\n",
    "        if self.site_only ==True:\n",
    "            for app_name in concat_pred_df.columns:\n",
    "                app_series_values = concat_pred_df[app_name].values.flatten()\n",
    "                pred[app_name] = pd.Series(app_series_values)\n",
    "            pred_overall = pd.DataFrame(pred,dtype='float32')\n",
    "            pred_overall.plot(label=\"Pred\")\n",
    "            plt.title('Disaggregated Data')\n",
    "            plt.legend()\n",
    "\n",
    "        else:\n",
    "            for app_name in concat_pred_df.columns:\n",
    "                app_series_values = concat_pred_df[app_name].values.flatten()\n",
    "                # Neural nets do extra padding sometimes, to fit, so get rid of extra predictions\n",
    "                app_series_values = app_series_values[:len(gt_overall[app_name])]\n",
    "                pred[app_name] = pd.Series(app_series_values, index = gt_overall.index)\n",
    "            pred_overall = pd.DataFrame(pred,dtype='float32')\n",
    "        gt_df = pd.DataFrame(gt_overall)\n",
    "        pred_df = pd.DataFrame(pred_overall)\n",
    "#         print(gt_df)\n",
    "#         print(pred_df)\n",
    "        gt_df.to_csv('gt_csv_exp13.csv')\n",
    "        pred_df.to_csv('pred_csv_exp13.csv')\n",
    "        return gt_overall, pred_overall\n",
    "\n",
    "\n",
    "    # metrics\n",
    "    def compute_loss(self,gt,clf_pred, loss_function):\n",
    "        error = {}\n",
    "        for app_name in gt.columns:\n",
    "            error[app_name] = loss_function(gt[app_name],clf_pred[app_name])\n",
    "        return pd.Series(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint Testing for all algorithms\n",
      "Loading data for  REDD  dataset\n",
      "Loading data for meter ElecMeterID(instance=2, building=3, dataset='REDD')     \n",
      "Done loading data all meters for this chunk.\n",
      "Dropping missing values\n",
      "Generating predictions for : Seq2Point\n",
      "............  rmse  ..............\n",
      "        Seq2Point\n",
      "fridge  53.266028\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nilmtk.disaggregate import Mean\n",
    "from nilmtk_contrib.disaggregate import DAE,Seq2Point, Seq2Seq, RNN, WindowGRU\n",
    "\n",
    "\n",
    "experiment = {\n",
    "  'power': {'mains': ['apparent','active'],'appliance': ['apparent','active']},\n",
    "  'sample_rate': 60,\n",
    "  'appliances': ['fridge'],\n",
    "  'methods': {'Seq2Seq':Seq2Seq({'n_epochs':50,'batch_size':32})},\n",
    "  'train': {    \n",
    "    'datasets': {\n",
    "        'REDD': {\n",
    "            'path': 'converted_REDD.h5',\n",
    "            'buildings': {\n",
    "                2: {\n",
    "                    'start_time': '2011-04-21',\n",
    "                    'end_time': '2011-05-21'\n",
    "                    },\n",
    "                3: {\n",
    "                    'start_time': '2011-04-21',\n",
    "                    'end_time': '2011-05-21'\n",
    "                    },\n",
    "                5: {\n",
    "                    'start_time': '2011-04-21',\n",
    "                    'end_time': '2011-05-21'\n",
    "                    }\n",
    "                }                \n",
    "            }\n",
    "        }\n",
    "    },\n",
    "  'test': {\n",
    "    'datasets': {\n",
    "        'REDD': {\n",
    "            'path': 'converted_REDD.h5',\n",
    "            'buildings': {\n",
    "                1: {\n",
    "                    'start_time': '2011-04-21',\n",
    "                    'end_time': '2011-05-21'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'metrics':['rmse']\n",
    "    }\n",
    "}\n",
    "\n",
    "api_res = API(experiment)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5365e38e87508a924dd806c38ed66b8ea6d39e6367a7129b179e0b2ec997f787"
  },
  "kernelspec": {
   "display_name": "nilm",
   "language": "python",
   "name": "nilm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
